{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_schema = memory_schema = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"date\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The current date (YYYY-MM-DD HH-MM-SS format)\"\n",
    "    },\n",
    "    \"me\": {\n",
    "      \"type\": \"array\",\n",
    "      \"description\": \"My name\"\n",
    "    },\n",
    "    \"people\": {\n",
    "      \"type\": \"array\",\n",
    "      \"description\": \"List of people involved in the event (optional)\"\n",
    "    },\n",
    "    \"feeling\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The main character's feeling during the event\"\n",
    "    },\n",
    "    \"short_description\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"A brief description of the event\"\n",
    "    },\n",
    "    \"weather\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"Current weather conditions (e.g., sunny, rainy, cloudy)\"\n",
    "    },\n",
    "    \"location\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"Location name (e.g., city, town)\"\n",
    "    },\n",
    "    \"insight\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"Additional details or insights about the event\"\n",
    "    },\n",
    "    \"memorable_because\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The reason why the event is memorable\"\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "persona = \"Ros\"\n",
    "home_location = \"Milan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = groq.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are helpful memory recorder.\\nWrite outputs in JSON in schema: {my_schema}.\\nCurrent time is {now}.\\nI am {persona} living in {home_location} and events may take place in more specific places inside the home location or outside it, so record precisely.\\n\",\n",
    "            #\"content\": \"You are helpful memory recorder. Write outputs in JSON schema.\\n\",\n",
    "            #f\" The JSON object must use the schema: {json.dumps(my_schema.model_json_schema(), indent=1)}\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Today was sunny day and then rained, I went to city to have a dinner with friends and I ate the best Sushi I have ever tested in restaurant called Sushita Cafe, where my friend Paco is a chef.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\",\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"date\": \"2024-05-16 17:33:19\",\\n  \"me\": [\"Ros\"],\\n  \"people\": [\"Paco\"],\\n  \"feeling\": \"\",\\n  \"short_description\": \"Dinner with friends at Sushita Cafe\",\\n  \"weather\": \"Sunny and later rainy\",\\n  \"location\": \"Sushita Cafe, Milan\",\\n  \"insight\": \"Tasted the best Sushi ever\",\\n  \"memorable_because\": \"Delicious food and great company\"\\n}'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PydanticOutputParser' from 'langchain_core.output_parsers' (c:\\Users\\ELAFACRB1\\venvs\\langchain\\Lib\\site-packages\\langchain_core\\output_parsers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_groq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n",
      "File \u001b[1;32mc:\\Users\\ELAFACRB1\\venvs\\langchain\\Lib\\site-packages\\langchain_groq\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_groq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n\u001b[0;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGroq\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ELAFACRB1\\venvs\\langchain\\Lib\\site-packages\\langchain_groq\\chat_models.py:52\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     BaseChatModel,\n\u001b[0;32m     33\u001b[0m     agenerate_from_stream,\n\u001b[0;32m     34\u001b[0m     generate_from_stream,\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     37\u001b[0m     AIMessage,\n\u001b[0;32m     38\u001b[0m     AIMessageChunk,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     ToolMessageChunk,\n\u001b[0;32m     51\u001b[0m )\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m     JsonOutputParser,\n\u001b[0;32m     54\u001b[0m     PydanticOutputParser,\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutputParserLike\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     58\u001b[0m     JsonOutputKeyToolsParser,\n\u001b[0;32m     59\u001b[0m     PydanticToolsParser,\n\u001b[0;32m     60\u001b[0m     make_invalid_tool_call,\n\u001b[0;32m     61\u001b[0m     parse_tool_call,\n\u001b[0;32m     62\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'PydanticOutputParser' from 'langchain_core.output_parsers' (c:\\Users\\ELAFACRB1\\venvs\\langchain\\Lib\\site-packages\\langchain_core\\output_parsers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGroq(temperature=0, model_name=\"Llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Large Language Models (LLMs) have revolutionized the field of natural language processing (NLP) by enabling applications such as language translation, text summarization, and chatbots. However, traditional LLMs often suffer from high latency, which can be a significant limitation in many real-world applications. Low latency LLMs, on the other hand, offer several advantages that make them crucial for various use cases. Here are some reasons why low latency LLMs are important:\\n\\n1. **Real-time processing**: Low latency LLMs enable real-time processing of user input, which is essential for applications like chatbots, virtual assistants, and live language translation. This allows for a more seamless and responsive user experience.\\n2. **Improved user engagement**: Fast response times can significantly improve user engagement and satisfaction. Low latency LLMs can quickly respond to user queries, reducing the likelihood of user frustration and abandonment.\\n3. **Enhanced decision-making**: In applications like customer service or technical support, low latency LLMs can provide instant responses to user queries, enabling faster issue resolution and improved customer satisfaction.\\n4. **Increased efficiency**: Low latency LLMs can process large volumes of data quickly, making them suitable for applications like text classification, sentiment analysis, and topic modeling.\\n5. **Scalability**: Low latency LLMs can handle a high volume of concurrent requests, making them suitable for large-scale applications like social media platforms, online forums, and messaging apps.\\n6. **Improved accuracy**: Low latency LLMs can leverage real-time feedback and adapt to user behavior, leading to improved accuracy and more effective language understanding.\\n7. **Competitive advantage**: In today's fast-paced digital landscape, low latency LLMs can be a key differentiator for companies looking to stay ahead of the competition. By providing a faster and more responsive user experience, businesses can gain a competitive edge.\\n8. **Enhanced accessibility**: Low latency LLMs can improve accessibility for users with disabilities, such as those with cognitive or motor impairments, by providing faster and more intuitive interactions.\\n9. **Real-time analytics**: Low latency LLMs can enable real-time analytics and monitoring, allowing for more effective decision-making and optimization of language-based applications.\\n10. **Future-proofing**: As the demand for real-time language processing continues to grow, low latency LLMs are essential for building applications that can keep pace with the evolving needs of users.\\n\\nIn summary, low latency LLMs are crucial for building applications that require fast and responsive language processing, enabling real-time interactions, and providing a competitive edge in today's digital landscape.\", response_metadata={'token_usage': {'completion_time': 0.629, 'completion_tokens': 530, 'prompt_time': 0.013, 'prompt_tokens': 32, 'queue_time': None, 'total_time': 0.642, 'total_tokens': 562}, 'model_name': 'Llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-499d7e16-9c2f-40f3-a511-3ebcbda44a22-0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with streaming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see: https://python.langchain.com/v0.1/docs/integrations/chat/groq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) have revolutionized the field of natural language processing (NLP) by enabling applications such as language translation, text summarization, and chatbots. However, traditional LLMs often suffer from high latency, which can be a significant limitation in many real-world applications. Low latency LLMs, on the other hand, offer several advantages that make them crucial for various use cases. Here are some reasons why low latency LLMs are important:\n",
      "\n",
      "1. **Real-time processing**: Low latency LLMs enable real-time processing of user input, which is essential for applications like chatbots, virtual assistants, and live language translation. This allows for a more seamless and responsive user experience.\n",
      "2. **Improved user engagement**: Fast response times can significantly improve user engagement and satisfaction. Low latency LLMs can process user input quickly, reducing the likelihood of user frustration and abandonment.\n",
      "3. **Enhanced decision-making**: In applications like customer service or technical support, low latency LLMs can provide instant responses to user queries, enabling faster issue resolution and improved customer satisfaction.\n",
      "4. **Increased efficiency**: Low latency LLMs can automate routine tasks, freeing up human agents to focus on more complex and high-value tasks. This can lead to increased efficiency and reduced operational costs.\n",
      "5. **Competitive advantage**: In industries like finance, healthcare, or e-commerce, low latency LLMs can provide a competitive advantage by enabling faster and more accurate processing of user input, leading to improved customer experiences and increased loyalty.\n",
      "6. **Scalability**: Low latency LLMs can be designed to scale horizontally, allowing them to handle a large volume of user requests without compromising performance. This makes them suitable for large-scale applications.\n",
      "7. **Improved accuracy**: Low latency LLMs can leverage real-time feedback to improve their accuracy and adapt to changing user behavior, leading to better overall performance and more accurate responses.\n",
      "8. **Enhanced security**: Low latency LLMs can be designed with security in mind, enabling real-time threat detection and response to potential security breaches.\n",
      "9. **New use cases**: Low latency LLMs can enable new use cases that rely on real-time processing, such as:\n",
      "\t* Real-time language translation for international communication\n",
      "\t* Instant text summarization for news and research\n",
      "\t* Real-time sentiment analysis for customer feedback\n",
      "10. **Future-proofing**: As the demand for real-time processing continues to grow, low latency LLMs can future-proof applications and ensure they remain competitive in the market.\n",
      "\n",
      "In summary, low latency LLMs are crucial for applications that require real-time processing, improved user engagement, and enhanced decision-making. They offer a competitive advantage, improved accuracy, and scalability, making them an essential component of many industries and applications."
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream({\"text\": \"Explain the importance of low latency LLMs.\"}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils import database_managers, embedding\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embedding.EmbeddingFunction('openAI').embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection test already exists!\n"
     ]
    }
   ],
   "source": [
    "vectore_store=qdrantClient = database_managers.QDrantDBManager(\n",
    "    url=os.getenv('QDRANT_URL'),\n",
    "    port=6333,\n",
    "    collection_name=os.getenv('COLLECTION_NAME'),\n",
    "    vector_size=1536, #??\n",
    "    embedding=embedding,\n",
    "    record_manager_url=r\"sqlite:///record_manager_cache.sql\"\n",
    ")\n",
    "vectore_store_client=vectore_store.vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectore_store_client.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo codice in basso è utile per **fornire i documenti provenienti da un retrieval e fornirli ad un [prompt template](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/) !!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sembra che tu stia cercando ristoranti montani per un pranzo in alta quota. Ecco alcune opzioni che ti potrebbero interessare:\n",
      "\n",
      "1. RIFUGIO IL PASTORE: Situato in Valsesia, offre un pranzo montano con vista sulla natura e un menu che include specialità come la polenta della nonna e la tartare di fassona. Il prezzo è di circa 20-25 euro a testa.\n",
      "2. AGRITURISMO LULOC: Situato in Valtellina, offre un pranzo con menu fisso e vista sulla natura. Il prezzo è di circa 35 euro a testa.\n",
      "3. OSTERIA LA VIGNETTA: Situata non lontano dal lago di Garda, offre un pranzo con menu tradizionale mantovano e vista sulla natura. Il prezzo è di circa 40 euro a testa.\n",
      "4. KRO: Situato in zona Ponte di Legno, offre un pranzo con menu basato su ingredienti del territorio e vista sulla natura. Il prezzo è di circa 40-50 euro a testa.\n",
      "\n",
      "Spero che queste opzioni ti siano state utili!"
     ]
    }
   ],
   "source": [
    "# RAG Setup\n",
    "def llmama3_llm(question, context):\n",
    "    chat = ChatGroq(temperature=0, model_name=\"Llama3-8b-8192\")\n",
    "    system = \"You are a helpful assistant.\"\n",
    "    \n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"user\", formatted_prompt)])\n",
    "\n",
    "    chain = prompt | chat\n",
    "    return chain.stream({\"text\": question})\n",
    "    \n",
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "def rag_chain(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = combine_docs(retrieved_docs)\n",
    "    return llmama3_llm(question, formatted_context)\n",
    "\n",
    "for chunk in rag_chain(\"Dove posso fare un pranzo montanaro?\"):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "1. [RAG with Llama3 and Langchain](https://medium.com/@nookalabadrinath/rag-and-its-application-using-llama3-lang-chain-and-chroma-db-ec61e905b9a5)\n",
    "2. [LLmama3 via Groq](https://tmmtt.medium.com/llama-3-via-groq-api-9d4e5cef3640)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
