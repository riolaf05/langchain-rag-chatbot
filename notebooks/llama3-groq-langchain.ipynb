{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_schema = memory_schema = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"date\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The current date (YYYY-MM-DD HH-MM-SS format)\"\n",
    "    },\n",
    "    \"me\": {\n",
    "      \"type\": \"array\",\n",
    "      \"description\": \"My name\"\n",
    "    },\n",
    "    \"people\": {\n",
    "      \"type\": \"array\",\n",
    "      \"description\": \"List of people involved in the event (optional)\"\n",
    "    },\n",
    "    \"feeling\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The main character's feeling during the event\"\n",
    "    },\n",
    "    \"short_description\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"A brief description of the event\"\n",
    "    },\n",
    "    \"weather\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"Current weather conditions (e.g., sunny, rainy, cloudy)\"\n",
    "    },\n",
    "    \"location\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"Location name (e.g., city, town)\"\n",
    "    },\n",
    "    \"insight\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"Additional details or insights about the event\"\n",
    "    },\n",
    "    \"memorable_because\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The reason why the event is memorable\"\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "persona = \"Ros\"\n",
    "home_location = \"Milan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = groq.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are helpful memory recorder.\\nWrite outputs in JSON in schema: {my_schema}.\\nCurrent time is {now}.\\nI am {persona} living in {home_location} and events may take place in more specific places inside the home location or outside it, so record precisely.\\n\",\n",
    "            #\"content\": \"You are helpful memory recorder. Write outputs in JSON schema.\\n\",\n",
    "            #f\" The JSON object must use the schema: {json.dumps(my_schema.model_json_schema(), indent=1)}\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Today was sunny day and then rained, I went to city to have a dinner with friends and I ate the best Sushi I have ever tested in restaurant called Sushita Cafe, where my friend Paco is a chef.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\",\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"date\": \"2024-05-16 17:33:19\",\\n  \"me\": [\"Ros\"],\\n  \"people\": [\"Paco\"],\\n  \"feeling\": \"\",\\n  \"short_description\": \"Dinner with friends at Sushita Cafe\",\\n  \"weather\": \"Sunny and later rainy\",\\n  \"location\": \"Sushita Cafe, Milan\",\\n  \"insight\": \"Tasted the best Sushi ever\",\\n  \"memorable_because\": \"Delicious food and great company\"\\n}'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGroq(temperature=0, model_name=\"Llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Large Language Models (LLMs) have revolutionized the field of natural language processing (NLP) by enabling applications such as language translation, text summarization, and chatbots. However, traditional LLMs often suffer from high latency, which can be a significant limitation in many real-world applications. Low latency LLMs, on the other hand, offer several advantages that make them crucial for various use cases. Here are some reasons why low latency LLMs are important:\\n\\n1. **Real-time processing**: Low latency LLMs enable real-time processing of user input, which is essential for applications like chatbots, virtual assistants, and live language translation. This allows for a more seamless and responsive user experience.\\n2. **Improved user engagement**: Fast response times can significantly improve user engagement and satisfaction. Low latency LLMs can quickly respond to user queries, reducing the likelihood of user frustration and abandonment.\\n3. **Enhanced decision-making**: In applications like customer service or technical support, low latency LLMs can provide instant responses to user queries, enabling faster issue resolution and improved customer satisfaction.\\n4. **Increased efficiency**: Low latency LLMs can process large volumes of data quickly, making them suitable for applications like text classification, sentiment analysis, and topic modeling.\\n5. **Scalability**: Low latency LLMs can handle a high volume of concurrent requests, making them suitable for large-scale applications like social media platforms, online forums, and messaging apps.\\n6. **Improved accuracy**: Low latency LLMs can leverage real-time feedback and adapt to user behavior, leading to improved accuracy and more effective language understanding.\\n7. **Competitive advantage**: In today's fast-paced digital landscape, low latency LLMs can be a key differentiator for companies looking to stay ahead of the competition. By providing a faster and more responsive user experience, businesses can gain a competitive edge.\\n8. **Enhanced accessibility**: Low latency LLMs can improve accessibility for users with disabilities, such as those with cognitive or motor impairments, by providing faster and more intuitive interactions.\\n9. **Real-time analytics**: Low latency LLMs can enable real-time analytics and monitoring, allowing for more effective decision-making and optimization of language-based applications.\\n10. **Future-proofing**: As the demand for real-time language processing continues to grow, low latency LLMs are essential for building applications that can keep pace with the evolving needs of users.\\n\\nIn summary, low latency LLMs are crucial for building applications that require fast and responsive language processing, enabling real-time interactions, and providing a competitive edge in today's digital landscape.\", response_metadata={'token_usage': {'completion_time': 0.629, 'completion_tokens': 530, 'prompt_time': 0.013, 'prompt_tokens': 32, 'queue_time': None, 'total_time': 0.642, 'total_tokens': 562}, 'model_name': 'Llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-499d7e16-9c2f-40f3-a511-3ebcbda44a22-0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with streaming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see: https://python.langchain.com/v0.1/docs/integrations/chat/groq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) have revolutionized the field of natural language processing (NLP) by enabling applications such as language translation, text summarization, and chatbots. However, traditional LLMs often suffer from high latency, which can be a significant limitation in many real-world applications. Low latency LLMs, on the other hand, offer several advantages that make them crucial for various use cases. Here are some reasons why low latency LLMs are important:\n",
      "\n",
      "1. **Real-time processing**: Low latency LLMs enable real-time processing of user input, which is essential for applications like chatbots, virtual assistants, and live language translation. This allows for a more seamless and responsive user experience.\n",
      "2. **Improved user engagement**: Fast response times can significantly improve user engagement and satisfaction. Low latency LLMs can process user input quickly, reducing the likelihood of user frustration and abandonment.\n",
      "3. **Enhanced decision-making**: In applications like customer service or technical support, low latency LLMs can provide instant responses to user queries, enabling faster issue resolution and improved customer satisfaction.\n",
      "4. **Increased efficiency**: Low latency LLMs can automate routine tasks, freeing up human agents to focus on more complex and high-value tasks. This can lead to increased efficiency and reduced operational costs.\n",
      "5. **Competitive advantage**: In industries like finance, healthcare, or e-commerce, low latency LLMs can provide a competitive advantage by enabling faster and more accurate processing of user input, leading to improved customer experiences and increased loyalty.\n",
      "6. **Scalability**: Low latency LLMs can be designed to scale horizontally, allowing them to handle a large volume of user requests without compromising performance. This makes them suitable for large-scale applications.\n",
      "7. **Improved accuracy**: Low latency LLMs can leverage real-time feedback to improve their accuracy and adapt to changing user behavior, leading to better overall performance and more accurate responses.\n",
      "8. **Enhanced security**: Low latency LLMs can be designed with security in mind, enabling real-time threat detection and response to potential security breaches.\n",
      "9. **New use cases**: Low latency LLMs can enable new use cases that rely on real-time processing, such as:\n",
      "\t* Real-time language translation for international communication\n",
      "\t* Instant text summarization for news and research\n",
      "\t* Real-time sentiment analysis for customer feedback\n",
      "10. **Future-proofing**: As the demand for real-time processing continues to grow, low latency LLMs can future-proof applications and ensure they remain competitive in the market.\n",
      "\n",
      "In summary, low latency LLMs are crucial for applications that require real-time processing, improved user engagement, and enhanced decision-making. They offer a competitive advantage, improved accuracy, and scalability, making them an essential component of many industries and applications."
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream({\"text\": \"Explain the importance of low latency LLMs.\"}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "from utils import database_managers, embedding\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.schema import SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 4993.22it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding = embedding.EmbeddingFunction('fast-bgeEmbedding').embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"web-places\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection web-places already exists!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELAFACRB1\\venvs\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 0.3.0. An updated version of the class exists in the langchain-qdrant package and should be used instead. To use it run `pip install -U langchain-qdrant` and import as `from langchain_qdrant import Qdrant`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "vectore_store=qdrantClient = database_managers.QDrantDBManager(\n",
    "    url=os.getenv('QDRANT_URL'),\n",
    "    port=6333,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vector_size=768, #??\n",
    "    embedding=embedding,\n",
    "    record_manager_url=r\"sqlite:///record_manager_cache.sql\"\n",
    ")\n",
    "vectore_store_client=vectore_store.vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectore_store_client.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for memory: https://python.langchain.com/v0.1/docs/expression_language/how_to/message_history/\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "store_key=\"key\"\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo codice in basso è utile per **fornire i documenti provenienti da un retrieval e fornirli ad un [prompt template](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/) !!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con memoria.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Setup\n",
    "def llmama3_llm(question, context):\n",
    "    chat = ChatGroq(temperature=0, model_name=\"Llama3-8b-8192\")\n",
    "    system = \"\"\"\n",
    "        Sei un assistente nella ricerca di ristoranti, locali, eventi. Date le informazioni di contesto restituisci una lista di ristoranti, locali, eventi richiesti,\n",
    "        che soddisfano i requisiti di ricerca.\n",
    "        Per ogni elemento indica:\n",
    "          + nome del ristorante, locale, evento\n",
    "          + indirizzo\n",
    "          + descrizione\n",
    "\n",
    "        Usa il seguente contesto:\n",
    "        {context}\n",
    "        \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", system),\n",
    "         MessagesPlaceholder(variable_name=\"history\"), \n",
    "         (\"human\", \"{question}\")\n",
    "         ])\n",
    "\n",
    "    runnable = prompt | chat\n",
    "    with_message_history = RunnableWithMessageHistory(\n",
    "        runnable,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"question\",\n",
    "        history_messages_key=\"history\",\n",
    "    )\n",
    "    \n",
    "    # return chain.stream({\"text\": question})\n",
    "    with_message_history.invoke(\n",
    "        {\"context\": context, \"question\": question},\n",
    "        config={\"configurable\": {\"session_id\": store_key}}\n",
    "    )\n",
    "    \n",
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "def rag_chain(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = combine_docs(retrieved_docs)\n",
    "    llmama3_llm(question, formatted_context)\n",
    "    return store[store_key].messages[-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Secondo il testo, se cerchi un ristorante in montagna, ti consiglio **Kro**, un ristorante situato nel piccolo paesino di Temù, in zona Ponte di Legno. Il menu si basa su ingredienti prevalentemente del territorio, con una bella rivisitazione dei piatti tipici di zona, senza tralasciare alcune proposte di pesce.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain(\"Dove posso mangiare in montagna?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrag_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDove posso fare un pranzo montanaro?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain(\"Dove posso fare un pranzo montanaro?\"):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "1. [RAG with Llama3 and Langchain](https://medium.com/@nookalabadrinath/rag-and-its-application-using-llama3-lang-chain-and-chroma-db-ec61e905b9a5)\n",
    "2. [LLmama3 via Groq](https://tmmtt.medium.com/llama-3-via-groq-api-9d4e5cef3640)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 2501.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection web-places already exists!\n"
     ]
    }
   ],
   "source": [
    "COLLECTION_NAME = \"web-places\"\n",
    "embedding = embedding.EmbeddingFunction('fast-bgeEmbedding').embedder\n",
    "vectore_store=qdrantClient = database_managers.QDrantDBManager(\n",
    "    url=os.getenv('QDRANT_URL'),\n",
    "    port=6333,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vector_size=768,\n",
    "    embedding=embedding,\n",
    "    record_manager_url=r\"sqlite:///record_manager_cache.sql\"\n",
    ")\n",
    "vectore_store_client=vectore_store.vector_store\n",
    "retriever = vectore_store_client.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Secondo il testo, se cerchi un ristorante in montagna, ti consiglio **Kro**, un ristorante situato nel piccolo paesino di Temù, in zona Ponte di Legno.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain(\"Dove posso mangiare in montagna?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
